# 1.组件版本 && 集群环境

## 组件版本

* Kubernetes 1.8.2(1.9.x版本也可以，只有细微的差别)
* Docker 17.10.0-ce
* Etcd 3.2.9
* Flanneld
* TLS 认证通信（所有组件，如etcd、kubernetes master 和node）
* RBAC 授权
* kubelet TLS Bootstrapping
* kubedns、dashboard、heapster等插件
* harbor，使用nfs后端存储


## etcd 集群 && k8s master 机器 && k8s node 机器

* **master01：192.168.1.137**
* **master02：192.168.1.138**
* **master03/node03：192.168.1.170**
* 由于机器有限，所以我们将master03 也作为node节点，后续有新的机器增加即可

## Vagrant 虚拟机模板

```
$ cd ~/k8s_nodes
$ vi Vagrantfile


Vagrant.require_version ">= 1.7.0"

$os_image = (ENV['OS_IMAGE'] || "ubuntu16").to_sym

Vagrant.configure("2") do |config|
  config.vm.provider "virtualbox"

  def set_vbox(vb, config)
    vb.gui = false
    vb.memory = 2048
    vb.cpus = 2

    case $os_image
    when :centos7
      config.vm.box = "bento/centos-7.3"
    when :ubuntu16
      config.vm.box = "bento/ubuntu-16.04"
    end
  end

  ips = ["192.168.1.137", "192.168.1.138", "192.168.1.170"]
  (0..2).each do |id|
    mid = id + 1
    config.vm.define "kube-node#{mid}" do |node|
      node.vm.hostname = "kube-node#{mid}"
      node.vm.network :private_network, ip: "#{ips[id]}",  auto_config: true
      #node.vm.network :public_network, ip: "#{ips[id]}",  auto_config: true

      node.vm.provider :virtualbox do |vb, override|
        vb.name = "kube-node#{mid}"
        set_vbox(vb, override)
      end
    end
  end
end
```

## 集群环境变量

后面的嗯部署将会使用到的全局变量，定义如下（根据自己的机器、网络修改）：

```
# TLS Bootstrapping 使用的Token，可以使用命令 head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 生成
BOOTSTRAP_TOKEN="8981b594122ebed7596f1d3b69c78223"

# 建议使用未用的网段来定义服务网段和Pod 网段
# 服务网段(Service CIDR)，部署前路由不可达，部署后集群内部使用IP:Port可达
SERVICE_CIDR="10.254.0.0/16"
# Pod 网段(Cluster CIDR)，部署前路由不可达，部署后路由可达(flanneld 保证)
CLUSTER_CIDR="172.30.0.0/16"

# 服务端口范围(NodePort Range)
NODE_PORT_RANGE="30000-32766"

# etcd集群服务地址列表
ETCD_ENDPOINTS="https://192.168.1.137:2379,https://192.168.1.138:2379,https://192.168.1.170:2379"

# flanneld 网络配置前缀
FLANNEL_ETCD_PREFIX="/kubernetes/network"

# kubernetes 服务IP(预先分配，一般为SERVICE_CIDR中的第一个IP)
CLUSTER_KUBERNETES_SVC_IP="10.254.0.1"

# 集群 DNS 服务IP(从SERVICE_CIDR 中预先分配)
CLUSTER_DNS_SVC_IP="10.254.0.2"

# 集群 DNS 域名
CLUSTER_DNS_DOMAIN="cluster.local."

# MASTER API Server 地址
MASTER_URL="k8s-api.virtual.local"
```

将上面变量保存为: **env.sh**，然后将脚本拷贝到所有机器的`/usr/k8s/bin`目录。

**在master01, 产生的`env.sh`, scp 到 master02和master03还有其他所有的node节点上, 要保持相同的`BOOTSTRAP_TOKEN`用于node加入cluster**

在所有节点上的操作：

```
sudo mkdir -p /usr/k8s/bin
sudo mv env.sh /usr/k8s/bin
source /usr/k8s/bin/env.sh
```

为方便后面迁移，我们在集群内定义一个域名用于访问`apiserver`，在每个节点的`/etc/hosts`文件中添加记录：

**`192.168.1.137 k8s-api.virtual.local k8s-api`**


其中`192.168.1.137`为master01 的IP，暂时使用该IP来做`apiserver`的负载地址

```
$ sudo vi /etc/hosts


192.168.1.137 k8s-api.virtual.local k8s-api
```